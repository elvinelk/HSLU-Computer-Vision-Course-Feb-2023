{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aac9dae-677e-40fd-84f1-40ed5a4176f4",
   "metadata": {},
   "source": [
    "# Mask Detection with Neural Networks\n",
    "\n",
    "Author: Umberto Michelucci - umberto.michelucci (at) gmail.com\n",
    "\n",
    "Version 1.0\n",
    "\n",
    "The code has been inspired, but simplified and modifed from https://www.pyimagesearch.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\levin\\anaconda3\\lib\\site-packages (2.11.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.27.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.20.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.48.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (20.9)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.10.0)\n",
      "\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\levin\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\levin\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-7d3019b0313c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mImageDataGenerator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapplications\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mMobileNetV2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAveragePooling2D\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDropout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mFlatten\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "INIT_LR = 1e-4 # learning rate\n",
    "EPOCHS = 10 # 10 are more than enough to get an accuracy of over 99%\n",
    "BS = 32 # Batchsize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "e0444fa5-af11-4d59-96a5-f8e07f2e5783",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use for this example data generated by Prajna Bhandary (you can get the data from her [GitHub repository](https://github.com/prajnasb/observations/tree/master/mask_classifier/Data_Generator)). This is a wonderful example of creativity in how to get enough data to train a deep learning model. The masks in the images are not real masks, but are artificially placed on the face of the person in the image, by identifying first the position of relevant landmarks of the face (nose, eyes, etc.) and then placing a \"fake\" masks on top of the face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698dfeba-0112-4eca-a348-cb951da1ee54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-54c07a8ac7c3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"[INFO] loading images...\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mimagePaths\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpaths\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlist_images\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"data\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# loop over the image paths\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'paths' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(\"data\"))\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    # Since we are going to use MobileNetV2 we need to resize the images\n",
    "    # to the expected size by the pre-trained network.\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccdbed-8a37-4b3e-bc17-86924970bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30035b93-c83a-4603-84f8-94e4820da9a6",
   "metadata": {},
   "source": [
    "Now we need to split the dataset to check overfitting and then we can create a `ImageDataGenerator` \n",
    "to do data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb251cd8-65ac-4c1a-a53b-669cf1feec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "# Split of data into train and test\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size=0.20, stratify=labels, random_state=42)\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d4278-d89c-44e7-b6b0-eddc8b5f8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load the MobilNetV2 by removing the last layers since\n",
    "# we want to do transfer learning.\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "# The following are the layers we will train\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# We don't want to train any layer in the MobileNetV2 network\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7d978-bc7a-4267-8f99-5dd83b939527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70e007-dafe-4d2d-a096-bfd1eee972af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile our model\n",
    "opt = Adam(learning_rate=INIT_LR, weight_decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ae82e-af29-40c0-bfec-be210ee1b762",
   "metadata": {},
   "source": [
    "A quick reminder\n",
    "\n",
    "- TP: True Positive\n",
    "- FP: False Positive\n",
    "- TN: True Negative\n",
    "- FN: False Negative\n",
    "\n",
    "\n",
    "$$\n",
    "\\textrm{PRECISION} = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\textrm{RECALL} = \\frac{TP}{P} = \\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72804cd4-fcd6-4127-8a74-7dce7330c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "    target_names=lb.classes_))\n",
    "model.save(\"model_mask_detection\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf859c-1f77-4e62-a7f3-54b9d3a97f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model_mask_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49341711-258a-4d7d-a61b-a89aac4495fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(\"losses.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4b025-8669-4e9a-9c59-6f016f10895d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8a8cc-67e2-413f-a36d-0f195ea28193",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = cv2.imread('test.jpg')\n",
    "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "face = cv2.resize(face, (224, 224))\n",
    "\n",
    "\n",
    "face2 = img_to_array(face)\n",
    "face2 = preprocess_input(face2)\n",
    "face2 = np.expand_dims(face2, axis=0)\n",
    "pred = np.argmin(model.predict(face2))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270a772-6bf0-4af4-8c8d-1c2d75c14209",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(face)\n",
    "plt.grid(False)\n",
    "title = 'With Mask' if (pred == 1) else 'Without Mask'\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4920892-d39d-445c-b572-9fd8ca22e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = cv2.imread('test_no.jpg')\n",
    "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "face = cv2.resize(face, (224, 224))\n",
    "\n",
    "\n",
    "face2 = img_to_array(face)\n",
    "face2 = preprocess_input(face2)\n",
    "face2 = np.expand_dims(face2, axis=0)\n",
    "pred = np.argmin(model.predict(face2))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28833c51-9b45-4583-8999-d28048172e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(face)\n",
    "plt.grid(False)\n",
    "title = 'With Mask' if (pred == 1) else 'Without Mask'\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abaef7b-0527-4f30-89c8-9a6a311e818e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef6921-8099-40f0-9b55-c9c2b6535456",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = cv2.imread('data/with_mask/0-with-mask.jpg')\n",
    "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "face = cv2.resize(face, (224, 224))\n",
    "\n",
    "\n",
    "face2 = img_to_array(face)\n",
    "face2 = preprocess_input(face2)\n",
    "face2 = np.expand_dims(face2, axis=0)\n",
    "pred = np.argmin(model.predict(face2))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efa624-7fd7-45c3-92b9-ba49a9b3bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(face)\n",
    "plt.grid(False)\n",
    "title = 'With Mask' if (pred == 1) else 'Without Mask'\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae9ddf-c17e-4c66-9144-70a8d906ba34",
   "metadata": {},
   "source": [
    "## Example without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d586938-c7e5-4c20-95cb-9bf633e10815",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = cv2.imread('data/without_mask/0.jpg')\n",
    "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "face = cv2.resize(face, (224, 224))\n",
    "\n",
    "\n",
    "face2 = img_to_array(face)\n",
    "face2 = preprocess_input(face2)\n",
    "face2 = np.expand_dims(face2, axis=0)\n",
    "pred = np.argmin(model.predict(face2))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3508add-4995-4906-852b-f233f0ddc381",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(face)\n",
    "plt.grid(False)\n",
    "title = 'With Mask' if (pred == 1) else 'Without Mask'\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c399ff9-85da-445d-8b73-d98a056d4311",
   "metadata": {},
   "source": [
    "# Improvements and Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212ee74-daaa-4f71-b3a1-ac4f1763c3d8",
   "metadata": {},
   "source": [
    "1. The training data is limited. More data with different pose and skin colors would increase the accuracy of the model. Can you create a larger dataset with the same approach?\n",
    "2. The model works best with white masks (the data is using only white masks). Generating images with masks of different colors would increase further the accuracy of the model. Try to trick the model by using different colors of masks. Does the model works equally good? Try to create a larger dataset by using masks of different colors and then train again the model. Can you get better results with masks that are not white?\n",
    "3. How could you get better? Maybe you could first localize the face and then apply the model you just trained only to the face (and not the entire image). But what happens if the mask covers so much face that you cannot localize it in the image?\n",
    "4. Can you make the model better to make it work with multiple persons in the image? You have to first implement point 3. and then do a loop over all the faces found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c49695-f740-41d2-ade5-4dd89111944b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "850bfe13401b9f01c0063d8d9bb81118391b9a46ef53c47b137bc7c8a44c74bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}